# Samantha Interface Assistant - AI Agent Guide

## Project Overview

**Samantha** is a Gradio-based interface for running open-source Large Language Models (LLMs) locally using llama.cpp/llama-cpp-python. Developed by MPC-ES (Ministério Público de Contas do Estado do Espírito Santo) under Open Science principles, it enables unlimited experimentation with GGUF models without internet connectivity or GPU requirements.

**Core Purpose**: Democratize AI by allowing users to chain prompts and models, execute generated code automatically, and observe token generation at a granular level ("Learning Mode").

## Architecture & Key Components

### Main Application (`app.py`)
- **4,335 lines** of extensively commented Python organized in numbered sections (search for `# ===` patterns)
- **5 nested loops** control text generation (see lines 789-1715):
  1. Loop 5 (FIFTH): Number of sequences
  2. Loop 1 (FIRST): Iterate over selected models
  3. Loop 2 (SECOND): Endless while loop for model reinitialization
  4. Loop 3 (THIRD): Iterate over prompt list
  5. Loop 4 (FOURTH): Token-by-token generation
- **Backend**: llama.cpp via `Llama` class instantiation (line 936)
- **Frontend**: Gradio `gr.Blocks` interface (line 3856) with extensive bilingual support (PT/EN)

### Virtual Environments
The project uses **two isolated conda environments**:
- **`samantha`**: Main runtime environment for `app.py` (see `requirements_samantha.txt`)
- **`jupyterlab`**: Isolated environment for executing AI-generated code (see `requirements_jupyterlab.txt`)

**Critical Pattern**: Code generated by models is executed in the `jupyterlab` environment to isolate execution risks from the main application.

### Activation Scripts
- **Windows-only**: `.bat` scripts for installation and activation
  - `install_samantha_ia.bat`: Full installation with miniconda setup
  - `activate_samantha.bat`: Activates both base and samantha environments
  - `open_samantha_ia.bat`: Launches the application
- **Linux users**: Manual conda environment setup required

## Critical Developer Workflows

### Running Samantha
```bash
# Windows
open_samantha_ia.bat

# Linux/Mac (manual)
conda activate /path/to/samantha/miniconda3/envs/samantha
python app.py
```

### Model Management
- Models must be **GGUF format** files placed in the user's `Downloads` folder
- Models are auto-discovered via `glob.glob(fr'{model_path}\*.gguf')` (line 447)
- Download models from Hugging Face using "Copy and Paste LLM" feature (paste URLs directly)
- Models are loaded one at a time to conserve memory

### Code Execution Flow
1. User prompts model → Model generates Python/HTML code
2. Code extraction via regex patterns (search for `re.findall` in `app.py`)
3. **Automatic execution** (if `run_code` checkbox enabled):
   - Python: Executed via subprocess in `jupyterlab` environment
   - HTML: Opened in browser tab
4. **Stop condition**: Print non-empty string (no error) OR return `"STOP_SAMANTHA"`
5. **Incremental coding**: Use deterministic settings (temperature=0) with Feedback Loop

### Prompt Chaining Syntax
```
[ Initial Prompt ]    # Prepended to each prompt
Main prompt here
$$$
Second prompt
$$$
[[ Final Prompt ]]    # Prepended to all responses

---                   # Line to ignore prompt
{temperature=0.7, top_p=0.9}  # Per-prompt hyperparameters
```

## Project-Specific Conventions

### Code Organization
- **Heavy commenting**: Nearly every line has inline explanations
- **Bilingual**: All UI strings in PT/EN via `language` dict (lines 177-412)
- **Section markers**: `# ===` for major sections, `# ---` for subsections
- **Global state**: 30+ global variables track interface state (lines 453-519)

### File Naming Patterns
- **Prompt files**: `system_prompts.txt`, `user_prompts.txt` (delimiter: `$-$-$` or `$$$`)
- **Data files**: `titanic.csv` for demos, `open_dtale.py` for D-Tale integration
- **Notebooks**: `EDA.ipynb` for exploratory data analysis, `TEST.ipynb` for experiments
- **Sound assets**: `click.mp3`, `notification.mp3` (pygame mixer)

### Code Execution Special Patterns
- **`#IDE` comment**: Mark code for extraction/execution when selected and copied
- **Empty string return `''`**: Suppress HTML popup window
- **`STOP_SAMANTHA` return**: Force exit from generation loop
- **HTML output**: Automatically displayed for non-empty terminal prints

## Integration Points

### External Dependencies
- **llama-cpp-python**: Model loading and inference (requires CMake + MSVC on Windows)
- **Gradio 4.25.0**: Web interface with custom CSS and JavaScript
- **pyttsx3**: Offline TTS via SAPI5 voices
- **Vosk**: Offline speech-to-text (English/Portuguese)
- **pygame**: Audio playback for notifications
- **PyMuPDF (fitz)**: PDF text extraction

### Data Analysis Stack (jupyterlab env)
Includes 25+ libraries: pandas, numpy, matplotlib, seaborn, plotly, scikit-learn, streamlit, dash, selenium, playwright, beautifulsoup4, pytesseract, pyautogui, etc.

### Cross-Environment Communication
- Main app (`samantha` env) → spawns subprocess → code execution (`jupyterlab` env)
- Temporary files for code execution in `tempfile.gettempdir()`
- Clipboard integration via `pyperclip` for code copying

## Critical Gotchas

1. **Windows-centric**: Batch files, SAPI5 voices, `winsound` module
2. **Path handling**: Uses raw strings `fr"{DIRETORIO_LOCAL}\file.ext"` with backslashes
3. **Model unloading**: Must explicitly unload models before changing `n_ctx` parameter
4. **Memory management**: Limited chat history (only previous response) to reduce context size
5. **Error handling**: Main loop wrapped in try/except to keep interface running (line 4315)
6. **Gradio context**: Interface elements must be defined within `gr.Blocks()` context

## Key Files Reference

- **`app.py`**: Monolithic main application (start reading from section markers)
- **`requirements_samantha.txt`**: Main environment dependencies (113 lines)
- **`requirements_jupyterlab.txt`**: Code execution environment
- **`system_prompts.txt`**: Example system prompts (delimiter: `$-$-$`)
- **`user_prompts.txt`**: Example user prompts with chaining syntax (512 lines)
- **`README.md`**: Comprehensive documentation (3,306 lines) with video examples
- **`images/`**: UI assets (banner, logos, icons)

## Development Tips

- **Debug mode**: Set breakpoints in functions called by Gradio (global code doesn't support breakpoints)
- **VSCode shortcuts**: `Ctrl+K, Ctrl+0` collapse all, `Ctrl+K, Ctrl+J` expand all
- **Testing models**: Use small quantized models (Q4_K_M) from bartowski on Hugging Face
- **Feedback Loop**: Essential for incremental coding and AI-AI conversations
- **Learning Mode**: Set delay to observe token selection probabilities in real-time

## Example Workflows

### AI Judge Challenge (AI-AI Dialog)
1. Model 1 (Judge): Creates challenging question
2. Model 2 (AI 1): Responds as non-human entity
3. Model 3 (AI 2): Responds as non-human entity  
4. Model 1 (Judge): Evaluates responses
*See `user_prompts.txt` lines 1-50 for full examples*

### Incremental Data Analysis
1. Enable: Feedback Loop + Run Code Automatically + Cumulative Response
2. Prompt 1: "Load dataset and show info"
3. Prompt 2: "Generate scatter plots for all column pairs"
4. Each response builds on previous code execution results

### Automated Workflow Creation
1. Generate code → `Ctrl+C` → Click "Save Python File" → Enter prefix (e.g., "01")
2. Repeat for subsequent steps (02, 03, etc.)
3. Select all files → Click "Run Files" → Executes in numerical order

## When Modifying Code

- **Respect the 5-loop structure**: Changes to generation logic affect lines 789-1715
- **Update both languages**: Modify PT/EN strings in `language` dict simultaneously  
- **Test with small models**: Use 2-3B parameter models for rapid iteration
- **Check global state**: 30+ globals (lines 453-519) track UI state—ensure consistency
- **Preserve comments**: Extensive documentation aids future contributors

---

*Last updated: November 2025 | Version: 0.18.0 | License: MIT*
