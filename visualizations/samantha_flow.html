<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>Samantha Prompt & llama.cpp Anatomy</title>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
<style>
    :root {
        color-scheme: dark;
        --bg: #050914;
        --panel: #11182b;
        --border: rgba(255, 255, 255, 0.08);
        --text: #f8fafc;
        --muted: #94a3b8;
    }
    * { box-sizing: border-box; }
    body {
        font-family: "Inter", "Segoe UI", system-ui, -apple-system, sans-serif;
        margin: 0;
        padding: 1.5rem;
        background: radial-gradient(circle at top, #1e3a8a 0%, var(--bg) 45%);
        color: var(--text);
        min-height: 100vh;
        display: flex;
        flex-direction: column;
        gap: 1.5rem;
    }
    header, section {
        width: min(1200px, 100%);
        margin: 0 auto;
        background: rgba(8, 13, 31, 0.9);
        border: 1px solid var(--border);
        border-radius: 20px;
        padding: 1.5rem;
        box-shadow: 0 30px 60px rgba(0, 0, 0, 0.45);
        backdrop-filter: blur(12px);
    }
    h1 { margin: 0 0 0.6rem; font-size: clamp(1.4rem, 4vw, 2.2rem); }
    p.lead { margin: 0; color: var(--muted); }
    svg {
        width: 100%;
        height: 640px;
        border-radius: 16px;
        background: repeating-linear-gradient(90deg, rgba(255,255,255,0.02) 0, rgba(255,255,255,0.02) 16px, transparent 16px, transparent 32px);
        border: 1px solid rgba(255,255,255,0.05);
    }
    .lane-label {
        font-size: 0.9rem;
        fill: var(--muted);
        text-anchor: middle;
    }
    .node circle {
        r: 34;
        stroke: rgba(3, 7, 18, 0.9);
        stroke-width: 3px;
        cursor: pointer;
    }
    .node text {
        fill: var(--text);
        pointer-events: none;
        text-anchor: middle;
        font-size: 0.72rem;
    }
    .link {
        fill: none;
        stroke-width: 2.5px;
        opacity: 0.75;
    }
    .dimmed { opacity: 0.08 !important; }
    #legend {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));
        gap: 0.8rem;
        margin-top: 1rem;
    }
    .legend-card {
        padding: 1rem;
        border-radius: 14px;
        border: 1px solid var(--border);
        background: rgba(15, 23, 42, 0.8);
        position: relative;
    }
    .legend-card::before {
        content: "";
        width: 14px;
        height: 14px;
        border-radius: 4px;
        position: absolute;
        top: 1rem;
        right: 1rem;
        background: var(--card-color, #fff);
        box-shadow: 0 0 12px var(--card-color, #fff);
    }
    .legend-card h3 { margin: 0 0 0.3rem; font-size: 0.95rem; }
    .legend-card p { margin: 0; font-size: 0.85rem; color: var(--muted); }
    .timeline {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(240px, 1fr));
        gap: 1rem;
        margin-top: 1rem;
    }
    .timeline-step {
        background: rgba(15, 23, 42, 0.85);
        border: 1px solid var(--border);
        border-radius: 16px;
        padding: 1rem 1.2rem;
    }
    .timeline-step strong {
        display: block;
        font-size: 0.95rem;
        margin-bottom: 0.35rem;
    }
    #tooltip {
        position: absolute;
        pointer-events: none;
        padding: 0.9rem 1rem;
        background: rgba(7, 11, 25, 0.95);
        border-radius: 12px;
        border: 1px solid var(--border);
        max-width: 320px;
        font-size: 0.85rem;
        line-height: 1.3;
        box-shadow: 0 20px 40px rgba(0, 0, 0, 0.55);
        opacity: 0;
    }
</style>
</head>
<body>
<header>
    <h1>Samantha Prompt Chaining → llama.cpp → Execution</h1>
    <p class="lead">
        Follow the data like a metabolic pathway: prompts enter on the left, march down the five-loop ladder, flow through llama.cpp,
        and feed downstream code execution plus feedback reservoirs. Hover nodes to read what each stage contributes; the lanes give a
        strict left-to-right dependency so you can trace causality.
    </p>
</header>

<section>
    <svg id="diagram"></svg>
</section>

<section>
    <h2 style="margin-top:0">Subsystem Cards</h2>
    <div id="legend"></div>
</section>

<section>
    <h2 style="margin-top:0">Loop Timeline (think of it as sequential reactions)</h2>
    <div class="timeline">
        <div class="timeline-step">
            <strong>Step 1 · Intake & Cueing</strong>
            Load TXT prompts, split by $$$, extract hyperparameters, and pull GGUF models from Downloads or URLs.
        </div>
        <div class="timeline-step">
            <strong>Step 2 · Loop Ladder</strong>
            Loop 5 multiplies the model list, Loop 1 iterates models, Loop 2 reloads llama.cpp per model, Loop 3 iterates prompts, Loop 4 streams tokens.
        </div>
        <div class="timeline-step">
            <strong>Step 3 · llama.cpp Core</strong>
            Llama() sets n_ctx/threading, tokenizer builds KV cache, sampling stack outputs tokens, Learning Mode probes logits.
        </div>
        <div class="timeline-step">
            <strong>Step 4 · Interpreter Handoff</strong>
            Regex extracts Python/HTML, #IDE clipboard wins priority, subprocesses in the jupyterlab env run the code, stop signals propagate back.
        </div>
        <div class="timeline-step">
            <strong>Step 5 · Feedback Reservoir</strong>
            Outputs hit Gradio, audio, HTML, and full_text/partial_text files so the next prompt can recycle these metabolites.
        </div>
    </div>
</section>

<div id="tooltip"></div>

<script>
const stages = [
    { id: "Prompt Intake", color: "#FFB703", blurb: "Reads prompt files, applies bracket wrappers, extracts per-prompt hyperparameters." },
    { id: "Loop Ladder", color: "#FB8500", blurb: "Five nested loops orchestrate model × prompt × token sequencing." },
    { id: "llama.cpp Core", color: "#219EBC", blurb: "Llama() init, tokenizer/KV cache, sampling stack, and Learning Mode analytics." },
    { id: "Execution Bubble", color: "#8ECAE6", blurb: "Regex extraction, clipboard #IDE override, jupyterlab Python/HTML subprocesses, stop conditions." },
    { id: "Feedback & UI", color: "#C77DFF", blurb: "HTML rendering, audio, Gradio widgets, and the text reservoirs feeding the next cycle." }
];

const nodes = [
    { id: "Prompt Files", stage: "Prompt Intake", detail: "system_prompts.txt, user_prompts.txt, and last_* backups are loaded at startup." },
    { id: "System Prompt Store", stage: "Prompt Intake", detail: "Maintains bilingual defaults plus [ ] / [[ ]] wrappers appended to each prompt." },
    { id: "Prompt Chaining Parser", stage: "Prompt Intake", detail: "Splits prompts using $$$, ignores --- lines, resolves partial_text/full_text placeholders." },
    { id: "Hyperparameter Extractor", stage: "Prompt Intake", detail: "Reads {temperature=0.7,...} blocks and overrides sliders per prompt." },
    { id: "Downloads + Copy&Paste", stage: "Prompt Intake", detail: "Scans ~/Downloads for *.gguf and downloads ad-hoc models into MODEL_FOR_TESTING.gguf." },
    { id: "Loop 5 · Sequence Count", stage: "Loop Ladder", detail: "Replicates the model list when Number of Loops > 1." },
    { id: "Loop 1 · Models", stage: "Loop Ladder", detail: "Iterates the GGUF list, respecting shuffle and single-answer rules." },
    { id: "Loop 2 · Reinit", stage: "Loop Ladder", detail: "Deletes the previous Llama() object, loads the next model, resets vocabulary cache." },
    { id: "Loop 3 · Prompts", stage: "Loop Ladder", detail: "Feeds each prompt chunk into the active model while tracking per-model response counts." },
    { id: "Loop 4 · Tokens", stage: "Loop Ladder", detail: "Streams llm.create_chat_completion(..., stream=True) token by token." },
    { id: "Llama Loader", stage: "llama.cpp Core", detail: "Instantiates Llama(model_path, n_ctx, n_threads, seed) around line 936." },
    { id: "Tokenizer & Context", stage: "llama.cpp Core", detail: "Tokenizes system prompt, previous response, and user prompt; grows KV cache." },
    { id: "Sampling Stack", stage: "llama.cpp Core", detail: "Applies temperature, top_p, min_p, tfs_z, repeat penalties before emitting each token." },
    { id: "Learning Mode", stage: "llama.cpp Core", detail: "Reads llm.eval_logits[-1] to visualize token probabilities and unlikely selections." },
    { id: "Python / HTML Extractor", stage: "Execution Bubble", detail: "Regex captures ```python```/```html``` blocks or #IDE-tagged selections." },
    { id: "Clipboard & #IDE", stage: "Execution Bubble", detail: "#IDE in copied text overrides fenced blocks when Run Code is triggered." },
    { id: "JupyterLab Runner", stage: "Execution Bubble", detail: "Writes temp files and runs python.exe inside miniconda3/envs/jupyterlab." },
    { id: "Stop Conditions", stage: "Execution Bubble", detail: "Detects STOP_SAMANTHA, non-empty stdout, empty-string HTML suppression, Stop buttons." },
    { id: "HTML Renderer", stage: "Feedback & UI", detail: "Shows HTML output in a browser tab whenever subprocess stdout contains markup." },
    { id: "Audio / TTS", stage: "Feedback & UI", detail: "winsound beeps, pyttsx3, and optional Edge TTS read-outs of ultima_resposta." },
    { id: "Gradio Output", stage: "Feedback & UI", detail: "Updates right-column history, Learning Mode charts, buttons, and voice widgets." },
    { id: "Feedback Reservoir", stage: "Feedback & UI", detail: "Writes full_text.txt/partial_text.txt and updates previous_answer for the next cycle." }
];

const links = [
    { source: "Prompt Files", target: "Prompt Chaining Parser", label: "load & split $$$" },
    { source: "System Prompt Store", target: "Prompt Chaining Parser", label: "prepend []/[[ ]]" },
    { source: "Prompt Chaining Parser", target: "Hyperparameter Extractor", label: "{...} blocks" },
    { source: "Hyperparameter Extractor", target: "Loop 5: Sequence Count", label: "per-sequence overrides" },
    { source: "Downloads/GGUF", target: "Loop 1: Model Iterator", label: "model list" },
    { source: "Loop 5: Sequence Count", target: "Loop 1: Model Iterator", label: "expanded sequence" },
    { source: "Loop 1: Model Iterator", target: "Loop 2: Reinit & Load", label: "per-model settings" },
    { source: "Loop 2: Reinit & Load", target: "Llama.cpp Loader", label: "instantiate" },
    { source: "Llama.cpp Loader", target: "Tokenizer & Context Window", label: "n_ctx, seed" },
    { source: "Tokenizer & Context Window", target: "Loop 3: Prompt Iterator", label: "tokenized prompt" },
    { source: "Loop 3: Prompt Iterator", target: "Loop 4: Token Stream", label: "messages[]" },
    { source: "Loop 4: Token Stream", target: "Sampling Stack", label: "logits" },
    { source: "Sampling Stack", target: "Learning Mode", label: "probability traces" },
    { source: "Sampling Stack", target: "Gradio UI Output", label: "streamed tokens" },
    { source: "Learning Mode", target: "Gradio UI Output", label: "bar charts" },
    { source: "Loop 4: Token Stream", target: "Python/HTML Extractor", label: "regex scan" },
    { source: "Python/HTML Extractor", target: "Clipboard & #IDE", label: "prioritize #IDE" },
    { source: "Clipboard & #IDE", target: "JupyterLab Runner", label: "temp files" },
    { source: "JupyterLab Runner", target: "Stop Conditions", label: "stdout/return" },
    { source: "Stop Conditions", target: "Loop 4: Token Stream", label: "interrupt" },
    { source: "Stop Conditions", target: "Gradio UI Output", label: "status" },
    { source: "Loop 4: Token Stream", target: "Feedback Loop State", label: "ultima_resposta" },
    { source: "Feedback Loop State", target: "Prompt Chaining Parser", label: "full_text / partial_text" },
    { source: "Feedback Loop State", target: "System Prompt Store", label: "previous_answer" },
    { source: "Loop 4: Token Stream", target: "Audio/TTS", label: "beeps + mp3" },
    { source: "Gradio UI Output", target: "Full Text Log", label: "write files" },
    { source: "Full Text Log", target: "Prompt Chaining Parser", label: "text substitution" },
    { source: "JupyterLab Runner", target: "HTML Renderer", label: "HTML stdout" },
    { source: "HTML Renderer", target: "Gradio UI Output", label: "iframe" }
];

const width = 1100;
const height = 640;
const svg = d3.select("#diagram")
    .attr("viewBox", `0 0 ${width} ${height}`)
    .attr("preserveAspectRatio", "xMidYMid meet");

const stageIndex = new Map(stages.map((stage, i) => [stage.id, i]));
const stageSpacing = width / (stages.length + 1);
const laneTop = 120;
const nodeSpacing = 90;
const laneCounts = new Map();

nodes.forEach(node => {
    const idx = stageIndex.get(node.stage);
    const count = laneCounts.get(node.stage) || 0;
    node.x = stageSpacing + idx * stageSpacing;
    node.y = laneTop + count * nodeSpacing;
    laneCounts.set(node.stage, count + 1);
});

svg.selectAll(".lane-label")
    .data(stages)
    .enter()
    .append("text")
    .attr("class", "lane-label")
    .attr("x", (_, i) => stageSpacing + i * stageSpacing)
    .attr("y", 60)
    .text(d => d.id);

const defs = svg.append("defs");
defs.append("marker")
    .attr("id", "arrow")
    .attr("viewBox", "0 -5 10 10")
    .attr("refX", 12)
    .attr("refY", 0)
    .attr("markerWidth", 6)
    .attr("markerHeight", 6)
    .attr("orient", "auto")
    .append("path")
    .attr("d", "M0,-5L10,0L0,5")
    .attr("fill", "#94a3b8");

const colorScale = d3.scaleOrdinal()
    .domain(stages.map(s => s.id))
    .range(stages.map(s => s.color));

const link = svg.append("g")
    .selectAll("path")
    .data(links.map(link => ({
        source: nodes.find(n => n.id === link.source),
        target: nodes.find(n => n.id === link.target),
        label: link.label
    })))
    .enter()
    .append("path")
    .attr("class", "link")
    .attr("stroke", d => d3.color(colorScale(d.source.stage)).brighter(0.5))
    .attr("marker-end", "url(#arrow)")
    .attr("d", d => curve(d.source, d.target));

function curve(source, target) {
    const midX = (source.x + target.x) / 2;
    return `M ${source.x} ${source.y} C ${midX} ${source.y}, ${midX} ${target.y}, ${target.x} ${target.y}`;
}

const node = svg.append("g")
    .selectAll("g")
    .data(nodes)
    .enter()
    .append("g")
    .attr("class", "node")
    .attr("transform", d => `translate(${d.x}, ${d.y})`);

node.append("circle").attr("fill", d => colorScale(d.stage));
node.append("text").text(d => d.id);

const tooltip = d3.select("#tooltip");

node
    .on("mouseover", function (event, d) {
        tooltip
            .style("opacity", 1)
            .html(`<strong>${d.id}</strong><br>${d.detail}`)
            .style("left", `${event.pageX + 12}px`)
            .style("top", `${event.pageY + 12}px`);
        highlight(d);
    })
    .on("mousemove", function (event) {
        tooltip
            .style("left", `${event.pageX + 12}px`)
            .style("top", `${event.pageY + 12}px`);
    })
    .on("mouseout", () => {
        tooltip.style("opacity", 0);
        resetHighlight();
    });

function highlight(nodeData) {
    node.classed("dimmed", d => d !== nodeData && !isNeighbor(nodeData, d));
    link.classed("dimmed", d => d.source !== nodeData && d.target !== nodeData);
}

function resetHighlight() {
    node.classed("dimmed", false);
    link.classed("dimmed", false);
}

const neighborSet = new Set();
links.forEach(({ source, target }) => {
    neighborSet.add(`${source}|${target}`);
    neighborSet.add(`${target}|${source}`);
});

function isNeighbor(a, b) {
    return neighborSet.has(`${a.id}|${b.id}`);
}

d3.select("#legend")
    .selectAll("div")
    .data(stages)
    .enter()
    .append("div")
    .attr("class", "legend-card")
    .style("--card-color", d => d.color)
    .style("border-color", d => `${d.color}33`)
    .style("box-shadow", d => `0 15px 35px ${d.color}22`)
    .html(d => `<h3>${d.id}</h3><p>${d.blurb}</p>`)
    .style("position", "relative")
    .style("background", d => `${d.color}0d`)
    .on("mouseover", (_, group) => {
        node.classed("dimmed", d => d.stage !== group.id);
        link.classed("dimmed", d => d.source.stage !== group.id && d.target.stage !== group.id);
    })
    .on("mouseout", () => resetHighlight());
</script>
</body>
</html>
